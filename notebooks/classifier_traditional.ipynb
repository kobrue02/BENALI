{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7ffc7e-fac7-4b12-b88a-42ee051df762",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7f19bb-5bc9-44af-8205-7496e95d529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf563f40-5639-430f-9886-29d43d6cb8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers==4.54.1 tokenizers==0.21.4 sentencepiece==0.2.0 tiktoken==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d4c161-cfd1-4cb6-a863-eac0b5e97d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5f2726-86a8-4918-81b3-e9f4f8a5c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc02ff1e-254c-4300-8fc6-08af295ab838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import nltk, string\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "class SentenceData(Dataset):\n",
    "    def __init__(self, df, fit_extractor=False, extractor=None, max_features=5000, ngram_range=(1,3)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: pandas DataFrame with 'sentence' and 'l1' columns\n",
    "            fit_extractor: if True, fit the feature extractor on this dataset\n",
    "            extractor: optional existing feature extractor (shared between splits)\n",
    "        \"\"\"\n",
    "        self.sentences = df[\"sentence\"].values.tolist()\n",
    "        self.labels = torch.tensor(df[\"l1\"].values.tolist(), dtype=torch.long)\n",
    "\n",
    "        # if no extractor provided, create one\n",
    "        if extractor is None:\n",
    "            self.extractor = self._build_extractor(max_features, ngram_range)\n",
    "        else:\n",
    "            self.extractor = extractor\n",
    "\n",
    "        # fit on training data\n",
    "        if fit_extractor:\n",
    "            self.extractor[\"tfidf_word\"].fit(self.sentences)\n",
    "            self.extractor[\"tfidf_char\"].fit(self.sentences)\n",
    "            self.extractor[\"count\"].fit(self.sentences)\n",
    "            ling_feats = self._extract_linguistic_features(self.sentences)\n",
    "            self.extractor[\"scaler\"].fit(ling_feats)\n",
    "\n",
    "        # transform into numeric features\n",
    "        self.features = self._transform(self.sentences)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "    # ---------------- internal helpers ----------------\n",
    "    def _build_extractor(self, max_features, ngram_range):\n",
    "        return {\n",
    "            \"tfidf_word\": TfidfVectorizer(max_features=max_features//4, ngram_range=(1,2), stop_words=\"english\"),\n",
    "            \"tfidf_char\": TfidfVectorizer(analyzer=\"char_wb\", max_features=max_features//4, ngram_range=(2,4)),\n",
    "            \"count\": CountVectorizer(max_features=max_features//4, ngram_range=ngram_range, stop_words=\"english\"),\n",
    "            \"scaler\": StandardScaler(),\n",
    "            \"ling_dim\": None\n",
    "        }\n",
    "\n",
    "    def _extract_linguistic_features(self, texts):\n",
    "        feats = []\n",
    "        for text in texts:\n",
    "            wc = len(text.split())\n",
    "            cc = len(text)\n",
    "            sent_count = len(nltk.sent_tokenize(text))\n",
    "            avg_word_len = np.mean([len(w) for w in text.split()]) if wc > 0 else 0\n",
    "\n",
    "            punct_count = sum(1 for c in text if c in string.punctuation)\n",
    "            digit_count = sum(1 for c in text if c.isdigit())\n",
    "            upper_count = sum(1 for c in text if c.isupper())\n",
    "\n",
    "            feats.append([\n",
    "                len(text), wc, sent_count, avg_word_len,\n",
    "                cc, punct_count, digit_count, upper_count,\n",
    "                punct_count / wc if wc else 0,\n",
    "                digit_count / cc if cc else 0,\n",
    "                upper_count / cc if cc else 0\n",
    "            ])\n",
    "        arr = np.array(feats)\n",
    "        if self.extractor[\"ling_dim\"] is None:\n",
    "            self.extractor[\"ling_dim\"] = arr.shape[1]\n",
    "        return arr\n",
    "\n",
    "    def _transform(self, texts):\n",
    "        tfidf_word = self.extractor[\"tfidf_word\"].transform(texts).toarray()\n",
    "        tfidf_char = self.extractor[\"tfidf_char\"].transform(texts).toarray()\n",
    "        count_feats = self.extractor[\"count\"].transform(texts).toarray()\n",
    "        ling_feats = self._extract_linguistic_features(texts)\n",
    "        ling_scaled = self.extractor[\"scaler\"].transform(ling_feats)\n",
    "\n",
    "        feats = np.hstack([tfidf_word, tfidf_char, count_feats, ling_scaled])\n",
    "        return torch.tensor(feats, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a334a5-81ab-4f2b-a5a5-b65bcda8b103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"nli_train_upsampled.csv\")\n",
    "val_df = pd.read_csv(\"nli_val.csv\")\n",
    "test_df = pd.read_csv(\"nli_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a94073-699a-4387-8c0e-c9c154bb7964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot label encodings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "train_df['l1'] = le.fit_transform(train_data['l1'])\n",
    "val_df['l1'] = le.transform(val_data['l1'])\n",
    "test_df['l1'] = le.transform(test_data['l1'])\n",
    "\n",
    "#for df in [train_data, val_data, test_data]:\n",
    "#    df = df.drop(columns=[\"l2\", \"source\", \"word_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480cd9b8-2157-4a06-b73e-ef6e91858d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset (fit extractor here)\n",
    "train_data = SentenceData(train_df, fit_extractor=True)\n",
    "\n",
    "# Share the same extractor for val/test\n",
    "val_data   = SentenceData(val_df, extractor=train_data.extractor)\n",
    "test_data  = SentenceData(test_df, extractor=train_data.extractor)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_data, batch_size=64)\n",
    "test_loader  = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "input_dim = train_data.features.shape[1]  # feature size for classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a737c14e-2ced-489f-b081-a449fec7c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class FeatureClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, n_classes, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6b8cf6-fac5-4552-adc5-3cfe80863ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "import uuid\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "def validate(clf, val_loader, criterion):\n",
    "    clf.eval()\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    val_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for val_sents, val_labels in val_loader:\n",
    "            val_outputs = clf(val_sents)\n",
    "            val_loss += criterion(val_outputs, val_labels).item()\n",
    "            val_pred = torch.argmax(val_outputs, dim=1)\n",
    "            val_acc += torch.sum(val_pred == val_labels).item()\n",
    "            val_samples += val_labels.size(0)\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_val_acc = val_acc / val_samples\n",
    "    \n",
    "    return avg_val_loss, avg_val_acc\n",
    "\n",
    "def train(clf, criterion, optimizer, train_loader, n_batches, epochs, run, log_freq, pbar):\n",
    "    clf.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for i, (sentences, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        outputs = clf(sentences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        pred = torch.argmax(outputs, dim=1)\n",
    "        train_acc = torch.sum(pred == labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += train_acc.item()\n",
    "        \n",
    "        if i % log_freq == 0:\n",
    "            run.log({\"batch_loss\": loss, \"batch_train_acc\": train_acc / labels.size(0)})\n",
    "        pbar.update(1)\n",
    "    \n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    avg_train_acc = epoch_acc / len(train_loader.dataset)\n",
    "    \n",
    "    return avg_train_loss, avg_train_acc\n",
    "\n",
    "def training_loop(clf, criterion, optimizer, train_loader, n_batches, epochs, val_loader, run, log_interval=100):\n",
    "    if hasattr(clf, 'fit_features') and not clf.fitted:\n",
    "        print(\"Fitting feature extractors on training data...\")\n",
    "        all_train_texts = []\n",
    "        for sentences, _ in tqdm(train_loader, desc=\"Collecting training texts\"):\n",
    "            all_train_texts.extend(sentences)\n",
    "        clf.fit_features(all_train_texts)\n",
    "        print(f\"Feature extraction complete. Total features: {clf.n_features}\")\n",
    "        \n",
    "    total_batches = len(train_loader) * epochs\n",
    "    pbar = tqdm(total=total_batches, desc='Training')\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        run.log({\"epoch\": e})\n",
    "        avg_train_loss, avg_train_acc = train(clf, criterion, optimizer, train_loader, n_batches, epochs, run, log_interval, pbar)\n",
    "        avg_val_loss, avg_val_acc = validate(clf, val_loader, criterion)\n",
    "        run.log({\n",
    "            \"epoch_train_loss\": avg_train_loss,\n",
    "            \"epoch_train_acc\": avg_train_acc,\n",
    "            \"epoch_val_loss\": avg_val_loss,\n",
    "            \"epoch_val_acc\": avg_val_acc\n",
    "        })\n",
    "        pbar.set_postfix({\n",
    "            'epoch': f'{e+1}/{epochs}',\n",
    "            'train_loss': f'{avg_train_loss:.4f}',\n",
    "            'val_loss': f'{avg_val_loss:.4f}',\n",
    "            'val_acc': f'{avg_val_acc:.4f}'\n",
    "        })\n",
    "    \n",
    "    pbar.close()\n",
    "\n",
    "def test_loop(clf, test_loader, le, run):\n",
    "    clf.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    target_names = [str(name) for name in le.classes_]\n",
    "    for i, (sentences, labels) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "        with torch.no_grad():\n",
    "            outputs = clf(sentences)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        y_true += labels.tolist()\n",
    "        y_pred += preds.tolist()\n",
    "    report = classification_report(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            target_names=target_names,\n",
    "            labels=list(range(len(target_names)))\n",
    "    )\n",
    "    ma_p, ma_r, ma_f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "    mi_p, mi_r, mi_f1, _ = precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "    w_p, w_r, w_f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    scores = {\n",
    "        \"test_macro_precision\": ma_p,\n",
    "        \"test_macro_recall\": ma_r,\n",
    "        \"test_macro_f1\": ma_f1,\n",
    "        \"test_micro_precision\": mi_p,\n",
    "        \"test_micro_recall\": mi_r,\n",
    "        \"test_micro_f1\": mi_f1,\n",
    "        \"test_weighted_precision\": w_p,\n",
    "        \"test_weighted_recall\": w_r,\n",
    "        \"test_weighted_f1\": w_f1\n",
    "    }\n",
    "    run.log(scores)\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print(f\"\\nTest Scores:\")\n",
    "    for key, value in scores.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    return report, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e415a13-8d4e-4af0-bf3e-b6e4e52bf11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "n_classes = len(le.classes_)\n",
    "n_layers = 2\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "epochs = 4\n",
    "\n",
    "clf = DeepTraditionalL1Classifier(n_classes=n_classes)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, num_workers=2, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, num_workers=2, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# wandb\n",
    "args = {\n",
    "    \"entity\": \"konrad-brg-university-of-t-bingen\",\n",
    "    \"project\": \"BENALI-Trad\",\n",
    "    \"config\": {\n",
    "        \"learning_rate\": lr,\n",
    "        \"architecture\": \"BERT+LINEAR\",\n",
    "        \"dataset\": \"dataset_clean.csv\",\n",
    "        \"epochs\": epochs,\n",
    "        \"log_interval\": 50,\n",
    "        \"n_batches\": len(train_loader),\n",
    "        \"criterion\": \"CrossEntropyLoss\"\n",
    "    },\n",
    "}\n",
    "\n",
    "train_config = {\n",
    "    \"clf\": clf,\n",
    "    \"optimizer\": AdamW(clf.parameters(), lr=lr),\n",
    "    \"criterion\": CrossEntropyLoss(),\n",
    "    \"train_loader\": train_loader,\n",
    "    \"val_loader\": val_loader,\n",
    "    \"n_batches\": len(train_loader),\n",
    "    \"epochs\": epochs,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d0c0d0-243a-46bd-982c-b2700ddcde43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%env TOKENIZERS_PARALLELISM=true\n",
    "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "log_interval = 100\n",
    "with wandb.init() as run:\n",
    "    run.watch(clf, log_freq=log_interval)\n",
    "    train_config.update({\"run\": run})\n",
    "    training_loop(**train_config)\n",
    "    report, scores = test_loop(clf, test_loader, le, run)\n",
    "    run.finish()\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1c9a98-cf83-40ae-a0ac-12c26579a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del clf\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140681b2-f0e3-4e3d-9367-d674da1c30d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/clf_3_layers_bert_base.json\", \"w\") as f:\n",
    "    json.dump(f, scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
